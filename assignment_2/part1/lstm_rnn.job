#!/bin/bash

#SBATCH --job-name=t30random
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --ntasks-per-node=1
#SBATCH --time=03:50:50
#sBATCH --mem=60000M
#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1

module purge
module load eb

module load Miniconda3/4.3.27
module load Python/3.6.3-foss-2017b
module load cuDNN/7.0.5-CUDA-9.0.176
module load NCCL/2.0.5-CUDA-9.0.176

export LD_LIBRARY_PATH=/hpc/eb/Debian9/cuDNN/7.1-CUDA-8.0.44-GCCcore-5.4.0/lib64:$LD_LI$

source activate dl

for i in {5..40}
do
   srun python3 -u train.py --input_length=$i --model_type='RNN' --learning_rate=0.001
done

for i in {5..40}
do
   srun python3 -u train.py --input_length=$i --model_type='LSTM' --learning_rate=0.01
done
